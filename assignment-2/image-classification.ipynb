{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the const variables\n",
    "\n",
    "# the path of the data\n",
    "DATA_PATH = (\n",
    "    \"assignment-2/dataset/Image_Classification/dataset_24581896\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing as preprocessing\n",
    "\n",
    "\n",
    "def prepare_data(data_path):\n",
    "    # load the data\n",
    "    train_ds = preprocessing.image_dataset_from_directory(\n",
    "        data_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(224, 224),\n",
    "        seed=24581896,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\"\n",
    "    )\n",
    "    \n",
    "    val_ds = preprocessing.image_dataset_from_directory(\n",
    "        data_path,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(224, 224),\n",
    "        seed=24581896,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\"\n",
    "    )\n",
    "\n",
    "    # set the class names\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    # set the number of classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    return train_ds, val_ds, class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, class_names, num_classes = prepare_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2. Create a Classifiers mdoel \n",
    "\n",
    "1. Customize AlexNet/GoogleNet/ResNet etc. and reduce/increase the layers, Train, and Test for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.applications as appl\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.optimizers as opt\n",
    "\n",
    "\n",
    "class AlexNet(models.Model):\n",
    "    \"\"\"\n",
    "    https://paperswithcode.com/method/alexnet\n",
    "    Neural network model consisting of layers propsed by AlexNet paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1000, input_shape=(224, 224, 3)):\n",
    "        \"\"\"\n",
    "        Define and allocate layers for this neural net.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): number of classes to predict with this model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # input size should be : (b x 3 x 227 x 227)\n",
    "        # The image in the original paper states that width and height are 224 pixels, but\n",
    "        # the dimensions after first convolution layer do not lead to 55 x 55.\n",
    "        self.net = models.Sequential(\n",
    "            # input layer\n",
    "            #\n",
    "            # first layer\n",
    "            layers.Conv2D(\n",
    "                filters=96,\n",
    "                kernel_size=11,\n",
    "                strides=4,\n",
    "                input_shape=input_shape,\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"),\n",
    "            # second layer\n",
    "            layers.Conv2D(\n",
    "                filters=256,\n",
    "                kernel_size=(11, 11),\n",
    "                strides=(1, 1), \n",
    "                padding=\"valid\",\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            # third layer\n",
    "            layers.Conv2D(\n",
    "                filters=384,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                padding='valid',\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            # layers.BatchNormalization(),\n",
    "            # layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "            # fourth layer\n",
    "            layers.Conv2D(\n",
    "                filters=384,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                padding=2,\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            # layers.BatchNormalization(),\n",
    "            # layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "            # fifth layer\n",
    "            layers.Conv2D(\n",
    "                filters=256,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                padding=2,\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        )\n",
    "        # classifier is just a name for linear layers\n",
    "        self.classifier = models.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(4096, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(4096, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.init_bias()  # initialize bias\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): output tensor\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the tensorflow model garden \n",
    "# https://github.com/tensorflow/models/tree/master/community\n",
    "# inception_v4\n",
    "# https://internal.dunescience.org/doxygen/inception__v4_8py_source.html\n",
    "\n",
    "\n",
    "def block_inception_a(inputs, scope=None, reuse=None):\n",
    "    \"\"\"Builds Inception-A block for Inception v4 network.\"\"\"\n",
    "    # By default use stride=1 and SAME padding\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding=\"SAME\"\n",
    "    ):\n",
    "        with tf.variable_scope(scope, \"BlockInceptionA\", [inputs], reuse=reuse):\n",
    "            with tf.variable_scope(\"Branch_0\"):\n",
    "                branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            with tf.variable_scope(\"Branch_1\"):\n",
    "                branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            with tf.variable_scope(\"Branch_2\"):\n",
    "                branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0c_3x3\")\n",
    "            with tf.variable_scope(\"Branch_3\"):\n",
    "                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "                branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n",
    "\n",
    "\n",
    "def block_reduction_a(inputs, scope=None, reuse=None):\n",
    "    \"\"\"Builds Reduction-A block for Inception v4 network.\"\"\"\n",
    "    # By default use stride=1 and SAME padding\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding=\"SAME\"\n",
    "    ):\n",
    "        with tf.variable_scope(scope, \"BlockReductionA\", [inputs], reuse=reuse):\n",
    "            with tf.variable_scope(\"Branch_0\"):\n",
    "                branch_0 = slim.conv2d(\n",
    "                    inputs,\n",
    "                    384,\n",
    "                    [3, 3],\n",
    "                    stride=2,\n",
    "                    padding=\"VALID\",\n",
    "                    scope=\"Conv2d_1a_3x3\",\n",
    "                )\n",
    "            with tf.variable_scope(\"Branch_1\"):\n",
    "                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "                branch_1 = slim.conv2d(\n",
    "                    branch_1,\n",
    "                    256,\n",
    "                    [3, 3],\n",
    "                    stride=2,\n",
    "                    padding=\"VALID\",\n",
    "                    scope=\"Conv2d_1a_3x3\",\n",
    "                )\n",
    "            with tf.variable_scope(\"Branch_2\"):\n",
    "                branch_2 = slim.max_pool2d(\n",
    "                    inputs, [3, 3], stride=2, padding=\"VALID\", scope=\"MaxPool_1a_3x3\"\n",
    "                )\n",
    "            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n",
    "\n",
    "\n",
    "def block_inception_b(inputs, scope=None, reuse=None):\n",
    "    \"\"\"Builds Inception-B block for Inception v4 network.\"\"\"\n",
    "    # By default use stride=1 and SAME padding\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding=\"SAME\"\n",
    "    ):\n",
    "        with tf.variable_scope(scope, \"BlockInceptionB\", [inputs], reuse=reuse):\n",
    "            with tf.variable_scope(\"Branch_0\"):\n",
    "                branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            with tf.variable_scope(\"Branch_1\"):\n",
    "                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "                branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "            with tf.variable_scope(\"Branch_2\"):\n",
    "                branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\"Conv2d_0b_7x1\")\n",
    "                branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\"Conv2d_0c_1x7\")\n",
    "                branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\"Conv2d_0d_7x1\")\n",
    "                branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\"Conv2d_0e_1x7\")\n",
    "            with tf.variable_scope(\"Branch_3\"):\n",
    "                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "                branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n",
    "\n",
    "\n",
    "def block_reduction_b(inputs, scope=None, reuse=None):\n",
    "    \"\"\"Builds Reduction-B block for Inception v4 network.\"\"\"\n",
    "    # By default use stride=1 and SAME padding\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding=\"SAME\"\n",
    "    ):\n",
    "        with tf.variable_scope(scope, \"BlockReductionB\", [inputs], reuse=reuse):\n",
    "            with tf.variable_scope(\"Branch_0\"):\n",
    "                branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_0 = slim.conv2d(\n",
    "                    branch_0,\n",
    "                    192,\n",
    "                    [3, 3],\n",
    "                    stride=2,\n",
    "                    padding=\"VALID\",\n",
    "                    scope=\"Conv2d_1a_3x3\",\n",
    "                )\n",
    "            with tf.variable_scope(\"Branch_1\"):\n",
    "                branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "                branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "                branch_1 = slim.conv2d(\n",
    "                    branch_1,\n",
    "                    320,\n",
    "                    [3, 3],\n",
    "                    stride=2,\n",
    "                    padding=\"VALID\",\n",
    "                    scope=\"Conv2d_1a_3x3\",\n",
    "                )\n",
    "            with tf.variable_scope(\"Branch_2\"):\n",
    "                branch_2 = slim.max_pool2d(\n",
    "                    inputs, [3, 3], stride=2, padding=\"VALID\", scope=\"MaxPool_1a_3x3\"\n",
    "                )\n",
    "            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n",
    "\n",
    "\n",
    "def block_inception_c(inputs, scope=None, reuse=None):\n",
    "    \"\"\"Builds Inception-C block for Inception v4 network.\"\"\"\n",
    "    # By default use stride=1 and SAME padding\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.avg_pool2d, slim.max_pool2d], stride=1, padding=\"SAME\"\n",
    "    ):\n",
    "        with tf.variable_scope(scope, \"BlockInceptionC\", [inputs], reuse=reuse):\n",
    "            with tf.variable_scope(\"Branch_0\"):\n",
    "                branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            with tf.variable_scope(\"Branch_1\"):\n",
    "                branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_1 = tf.concat(\n",
    "                    axis=3,\n",
    "                    values=[\n",
    "                        slim.conv2d(branch_1, 256, [1, 3], scope=\"Conv2d_0b_1x3\"),\n",
    "                        slim.conv2d(branch_1, 256, [3, 1], scope=\"Conv2d_0c_3x1\"),\n",
    "                    ],\n",
    "                )\n",
    "            with tf.variable_scope(\"Branch_2\"):\n",
    "                branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\"Conv2d_0b_3x1\")\n",
    "                branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\"Conv2d_0c_1x3\")\n",
    "                branch_2 = tf.concat(\n",
    "                    axis=3,\n",
    "                    values=[\n",
    "                        slim.conv2d(branch_2, 256, [1, 3], scope=\"Conv2d_0d_1x3\"),\n",
    "                        slim.conv2d(branch_2, 256, [3, 1], scope=\"Conv2d_0e_3x1\"),\n",
    "                    ],\n",
    "                )\n",
    "            with tf.variable_scope(\"Branch_3\"):\n",
    "                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "                branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n",
    "\n",
    "\n",
    "def inception_v4_base(inputs, final_endpoint=\"Mixed_7d\", scope=None):\n",
    "    \"\"\"Creates the Inception V4 network up to the given final endpoint.\n",
    "\n",
    "    Args:\n",
    "      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
    "      final_endpoint: specifies the endpoint to construct the network up to.\n",
    "        It can be one of [ 'Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n",
    "        'Mixed_3a', 'Mixed_4a', 'Mixed_5a', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d',\n",
    "        'Mixed_5e', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e',\n",
    "        'Mixed_6f', 'Mixed_6g', 'Mixed_6h', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c',\n",
    "        'Mixed_7d']\n",
    "      scope: Optional variable_scope.\n",
    "\n",
    "    Returns:\n",
    "      logits: the logits outputs of the model.\n",
    "      end_points: the set of end_points from the inception model.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if final_endpoint is not set to one of the predefined values,\n",
    "    \"\"\"\n",
    "    end_points = {}\n",
    "\n",
    "    def add_and_check_final(name, net):\n",
    "        end_points[name] = net\n",
    "        return name == final_endpoint\n",
    "\n",
    "    with tf.variable_scope(scope, \"InceptionV4\", [inputs]):\n",
    "        with slim.arg_scope(\n",
    "            [slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding=\"SAME\"\n",
    "        ):\n",
    "            # 299 x 299 x 3\n",
    "            net = slim.conv2d(\n",
    "                inputs, 32, [3, 3], stride=2, padding=\"VALID\", scope=\"Conv2d_1a_3x3\"\n",
    "            )\n",
    "            if add_and_check_final(\"Conv2d_1a_3x3\", net):\n",
    "                return net, end_points\n",
    "            # 149 x 149 x 32\n",
    "            net = slim.conv2d(net, 32, [3, 3], padding=\"VALID\", scope=\"Conv2d_2a_3x3\")\n",
    "            if add_and_check_final(\"Conv2d_2a_3x3\", net):\n",
    "                return net, end_points\n",
    "            # 147 x 147 x 32\n",
    "            net = slim.conv2d(net, 64, [3, 3], scope=\"Conv2d_2b_3x3\")\n",
    "            if add_and_check_final(\"Conv2d_2b_3x3\", net):\n",
    "                return net, end_points\n",
    "            # 147 x 147 x 64\n",
    "            with tf.variable_scope(\"Mixed_3a\"):\n",
    "                with tf.variable_scope(\"Branch_0\"):\n",
    "                    branch_0 = slim.max_pool2d(\n",
    "                        net, [3, 3], stride=2, padding=\"VALID\", scope=\"MaxPool_0a_3x3\"\n",
    "                    )\n",
    "                with tf.variable_scope(\"Branch_1\"):\n",
    "                    branch_1 = slim.conv2d(\n",
    "                        net,\n",
    "                        96,\n",
    "                        [3, 3],\n",
    "                        stride=2,\n",
    "                        padding=\"VALID\",\n",
    "                        scope=\"Conv2d_0a_3x3\",\n",
    "                    )\n",
    "                net = tf.concat(axis=3, values=[branch_0, branch_1])\n",
    "                if add_and_check_final(\"Mixed_3a\", net):\n",
    "                    return net, end_points\n",
    "\n",
    "            # 73 x 73 x 160\n",
    "            with tf.variable_scope(\"Mixed_4a\"):\n",
    "                with tf.variable_scope(\"Branch_0\"):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                    branch_0 = slim.conv2d(\n",
    "                        branch_0, 96, [3, 3], padding=\"VALID\", scope=\"Conv2d_1a_3x3\"\n",
    "                    )\n",
    "                with tf.variable_scope(\"Branch_1\"):\n",
    "                    branch_1 = slim.conv2d(net, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "                    branch_1 = slim.conv2d(\n",
    "                        branch_1, 96, [3, 3], padding=\"VALID\", scope=\"Conv2d_1a_3x3\"\n",
    "                    )\n",
    "                net = tf.concat(axis=3, values=[branch_0, branch_1])\n",
    "                if add_and_check_final(\"Mixed_4a\", net):\n",
    "                    return net, end_points\n",
    "\n",
    "            # 71 x 71 x 192\n",
    "            with tf.variable_scope(\"Mixed_5a\"):\n",
    "                with tf.variable_scope(\"Branch_0\"):\n",
    "                    branch_0 = slim.conv2d(\n",
    "                        net,\n",
    "                        192,\n",
    "                        [3, 3],\n",
    "                        stride=2,\n",
    "                        padding=\"VALID\",\n",
    "                        scope=\"Conv2d_1a_3x3\",\n",
    "                    )\n",
    "                with tf.variable_scope(\"Branch_1\"):\n",
    "                    branch_1 = slim.max_pool2d(\n",
    "                        net, [3, 3], stride=2, padding=\"VALID\", scope=\"MaxPool_1a_3x3\"\n",
    "                    )\n",
    "                net = tf.concat(axis=3, values=[branch_0, branch_1])\n",
    "                if add_and_check_final(\"Mixed_5a\", net):\n",
    "                    return net, end_points\n",
    "\n",
    "            # 35 x 35 x 384\n",
    "            # 4 x Inception-A blocks\n",
    "            for idx in range(4):\n",
    "                block_scope = \"Mixed_5\" + chr(ord(\"b\") + idx)\n",
    "                net = block_inception_a(net, block_scope)\n",
    "                if add_and_check_final(block_scope, net):\n",
    "                    return net, end_points\n",
    "\n",
    "            # 35 x 35 x 384\n",
    "            # Reduction-A block\n",
    "            net = block_reduction_a(net, \"Mixed_6a\")\n",
    "            if add_and_check_final(\"Mixed_6a\", net):\n",
    "                return net, end_points\n",
    "\n",
    "            # 17 x 17 x 1024\n",
    "            # 7 x Inception-B blocks\n",
    "            for idx in range(7):\n",
    "                block_scope = \"Mixed_6\" + chr(ord(\"b\") + idx)\n",
    "                net = block_inception_b(net, block_scope)\n",
    "                if add_and_check_final(block_scope, net):\n",
    "                    return net, end_points\n",
    "\n",
    "            # 17 x 17 x 1024\n",
    "            # Reduction-B block\n",
    "            net = block_reduction_b(net, \"Mixed_7a\")\n",
    "            if add_and_check_final(\"Mixed_7a\", net):\n",
    "                return net, end_points\n",
    "\n",
    "            # 8 x 8 x 1536\n",
    "            # 3 x Inception-C blocks\n",
    "            for idx in range(3):\n",
    "                block_scope = \"Mixed_7\" + chr(ord(\"b\") + idx)\n",
    "                net = block_inception_c(net, block_scope)\n",
    "                if add_and_check_final(block_scope, net):\n",
    "                    return net, end_points\n",
    "    raise ValueError(\"Unknown final endpoint %s\" % final_endpoint)\n",
    "\n",
    "\n",
    "def inception_v4(\n",
    "    inputs,\n",
    "    num_classes=1001,\n",
    "    is_training=True,\n",
    "    dropout_keep_prob=0.8,\n",
    "    reuse=None,\n",
    "    scope=\"InceptionV4\",\n",
    "    create_aux_logits=True,\n",
    "):\n",
    "    \"\"\"Creates the Inception V4 model.\n",
    "\n",
    "    Args:\n",
    "      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
    "      num_classes: number of predicted classes. If 0 or None, the logits layer\n",
    "        is omitted and the input features to the logits layer (before dropout)\n",
    "        are returned instead.\n",
    "      is_training: whether is training or not.\n",
    "      dropout_keep_prob: float, the fraction to keep before final layer.\n",
    "      reuse: whether or not the network and its variables should be reused. To be\n",
    "        able to reuse 'scope' must be given.\n",
    "      scope: Optional variable_scope.\n",
    "      create_aux_logits: Whether to include the auxiliary logits.\n",
    "\n",
    "    Returns:\n",
    "      net: a Tensor with the logits (pre-softmax activations) if num_classes\n",
    "        is a non-zero integer, or the non-dropped input to the logits layer\n",
    "        if num_classes is 0 or None.\n",
    "      end_points: the set of end_points from the inception model.\n",
    "    \"\"\"\n",
    "    end_points = {}\n",
    "    with tf.variable_scope(scope, \"InceptionV4\", [inputs], reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            net, end_points = inception_v4_base(inputs, scope=scope)\n",
    "\n",
    "            with slim.arg_scope(\n",
    "                [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                stride=1,\n",
    "                padding=\"SAME\",\n",
    "            ):\n",
    "                # Auxiliary Head logits\n",
    "                if create_aux_logits and num_classes:\n",
    "                    with tf.variable_scope(\"AuxLogits\"):\n",
    "                        # 17 x 17 x 1024\n",
    "                        aux_logits = end_points[\"Mixed_6h\"]\n",
    "                        aux_logits = slim.avg_pool2d(\n",
    "                            aux_logits,\n",
    "                            [5, 5],\n",
    "                            stride=3,\n",
    "                            padding=\"VALID\",\n",
    "                            scope=\"AvgPool_1a_5x5\",\n",
    "                        )\n",
    "                        aux_logits = slim.conv2d(\n",
    "                            aux_logits, 128, [1, 1], scope=\"Conv2d_1b_1x1\"\n",
    "                        )\n",
    "                        aux_logits = slim.conv2d(\n",
    "                            aux_logits,\n",
    "                            768,\n",
    "                            aux_logits.get_shape()[1:3],\n",
    "                            padding=\"VALID\",\n",
    "                            scope=\"Conv2d_2a\",\n",
    "                        )\n",
    "                        aux_logits = slim.flatten(aux_logits)\n",
    "                        aux_logits = slim.fully_connected(\n",
    "                            aux_logits,\n",
    "                            num_classes,\n",
    "                            activation_fn=None,\n",
    "                            scope=\"Aux_logits\",\n",
    "                        )\n",
    "                        end_points[\"AuxLogits\"] = aux_logits\n",
    "\n",
    "                # Final pooling and prediction\n",
    "                # TODO(sguada,arnoegw): Consider adding a parameter global_pool which\n",
    "                # can be set to False to disable pooling here (as in resnet_*()).\n",
    "                with tf.variable_scope(\"Logits\"):\n",
    "                    # 8 x 8 x 1536\n",
    "                    kernel_size = net.get_shape()[1:3]\n",
    "                    if kernel_size.is_fully_defined():\n",
    "                        net = slim.avg_pool2d(\n",
    "                            net, kernel_size, padding=\"VALID\", scope=\"AvgPool_1a\"\n",
    "                        )\n",
    "                    else:\n",
    "                        net = tf.reduce_mean(\n",
    "                            input_tensor=net,\n",
    "                            axis=[1, 2],\n",
    "                            keepdims=True,\n",
    "                            name=\"global_pool\",\n",
    "                        )\n",
    "                    end_points[\"global_pool\"] = net\n",
    "                    if not num_classes:\n",
    "                        return net, end_points\n",
    "                    # 1 x 1 x 1536\n",
    "                    net = slim.dropout(net, dropout_keep_prob, scope=\"Dropout_1b\")\n",
    "                    net = slim.flatten(net, scope=\"PreLogitsFlatten\")\n",
    "                    end_points[\"PreLogitsFlatten\"] = net\n",
    "                    # 1536\n",
    "                    logits = slim.fully_connected(\n",
    "                        net, num_classes, activation_fn=None, scope=\"Logits\"\n",
    "                    )\n",
    "                    end_points[\"Logits\"] = logits\n",
    "                    end_points[\"Predictions\"] = tf.nn.softmax(\n",
    "                        logits, name=\"Predictions\"\n",
    "                    )\n",
    "        return logits, end_points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
